#!/usr/bin/env python
# Author Asad Ismail
import torch
import torch.optim as optim
import time, sys, os, random
from tensorboardX import SummaryWriter
from util.config import cfg
from util.log import logger
import util.utils as utils
import torch.distributed as dist
from data_loader_util import synchronize, get_rank
from checkpoint import strip_prefix_if_present, align_and_update_state_dicts
from checkpoint import checkpoint
from solver import PolyLR
from torch.optim.lr_scheduler import StepLR
from model.pointgroup.pointgroup import PointGroup as Network
from model.pointgroup.pointgroup import model_fn_decorator
import os.path as osp
from tqdm import tqdm
from test import test as mapeval
from datetime import datetime
import json
import copy
import traceback

print(f"\n Succesffuly Imported required Packages!!")

# These are the paths to where SageMaker mounts interesting things in your container.
prefix = '/opt/ml/'

input_path = prefix + 'input/data/'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')
data_path  = os.path.join(prefix, 'input/config/inputdataconfig.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name_train='training'
training_path = os.path.join(input_path, channel_name_train)
channel_name_val='validation'
validation_path = os.path.join(input_path, channel_name_val)
logdir = output_path+"/logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
logger.info(f"Training path is {training_path}")
logger.info(f"Validation path is {validation_path}")


logger.info(f"The content of input path is {os.listdir(input_path)}")
logger.info(f"The content of input path is {os.listdir(training_path)}")


def init():
    # summary writer
    global writer
    writer = SummaryWriter(cfg.exp_path)
    gpu=0
    torch.cuda.set_device(gpu)
    torch.backends.cudnn.enabled=False


def train(train_loader, model, model_fn, optimizer, start_iter,end_epoch,scheduler, save_to_disc=True,data_name="planteye"):
    bestap=0
    # Early stopping
    total_patience=10
    current_patience=0
    iter_time = utils.AverageMeter()
    data_time = utils.AverageMeter()
    am_dict = {}
    model_fn_test = model_fn_decorator(test=True)
    model.train()
    start_time = time.time()
    end = time.time()
    data_len = len(train_loader)
    logger.info(f"Training for dataset length of {len(train_loader)}")
    for iteration, batch in tqdm(enumerate(train_loader, start_iter),total=cfg.max_iter):
        data_time.update(time.time() - end)
        torch.cuda.empty_cache()
        epoch = int(iteration / data_len)
        ##### adjust learning rate
        # utils.step_learning_rate(optimizer, cfg.lr, epoch - 1, cfg.step_epoch, cfg.multiplier)

        ##### prepare input and forward
        loss, _, visual_dict, meter_dict = model_fn(batch, model, iteration)

        ##### meter_dict
        for k, v in meter_dict.items():
            if k not in am_dict.keys():
                am_dict[k] = utils.AverageMeter()
            am_dict[k].update(v[0], v[1])

        ##### backward
        loss=loss.to(torch.float64)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()

        ##### time and print
        # current_iter = (epoch - 1) * len(train_loader) + i + 1
        current_iter = iteration
        max_iter = len(train_loader)
        remain_iter = max_iter - current_iter

        iter_time.update(time.time() - end)
        end = time.time()

        remain_time = remain_iter * iter_time.avg
        t_m, t_s = divmod(remain_time, 60)
        t_h, t_m = divmod(t_m, 60)
        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))

        if save_to_disc:
            logger.info(f"Training epoch {epoch}")
            logger.info("Iteration: {}/{}, lr:{:.6f}  train loss: {:.4f}, time: {}s".format(iteration, cfg.max_iter, scheduler.get_lr()[0], am_dict['loss'].avg, time.time() - start_time))
            # if epoch % cfg.save_freq == 0:
                # utils.checkpoint_save(model, cfg.output_path, cfg.config.split('/')[-1][:-5], epoch, cfg.save_freq, use_cuda)
            if (iteration % cfg.save_freq == 0 or iteration==cfg.max_iter-1) and save_to_disc:
                checkpoint(model, optimizer, iteration, cfg.output_path, None, None)

            for k in am_dict.keys():
                if k in visual_dict.keys():
                    writer.add_scalar(k+'_train', am_dict[k].avg, iteration)
        ### Should also include some prepare epoch before testing otherwise runs out of memory
        if (iteration % cfg.test_epoch == 0  or iteration==cfg.max_iter-1) and save_to_disc and iteration!=0:
            logger.info(f"Running Evaluation!!!")
            #model.attribute = list(model.attribute)  # where attribute was dict_keys
            #model_clone = copy.deepcopy(model)
            allap=mapeval(model,model_fn=model_fn_test,data_name=data_name,epoch=iteration)
            logger.info(f"val_acc:{allap}")
            logger.info(f"Current Best AP:{bestap}")
            if allap>bestap:
                current_patience=0
                bestap=allap
                logger.info(f"New BestAP Found!! BestAP is {bestap}")
                checkpoint(model, optimizer, iteration, cfg.output_path, None, None)
            else:
                current_patience+=1
            model=model.train()
            logger.info(f"Patience level {current_patience}/{total_patience}")
        if iteration>end_epoch:
            logger.info(f"Ended Training Successfully!!!")
            return
        if current_patience>total_patience:
            logger.info(f"Early stopping the training since accuracy is not improving for {total_patience} epochs!!!")
            return
        
            


def start_train():

    with open(param_path, 'r') as params:
        hyperParams = json.load(params)
    print(f"Hyper parameters: {hyperParams}")

    ##### init
    init()
    ##### get model version and data version
    exp_name = cfg.config.split('/')[-1][:-5]
    model_name = exp_name.split('_')[0]
    data_name = exp_name.split('_')[-1]
    
    int_params=["cluster_meanActive","cluster_shift_meanActive","cluster_npoint_thre"]
    float_params=["cluster_radius","lr"]
    for param_name,param_value in hyperParams.items():
        if hasattr(cfg, param_name):
            logger.info("Setting param {param_name} to {param_value}")
            if param_name in float_params:
                setattr(cfg, param_name,float(param_value))
            elif param_name in int_params:
                setattr(cfg,param_name,int(param_value))
            else:
                setattr(cfg, param_name,param_value)
    setattr(cfg,"output_path",model_path)
    setattr(cfg,"data_root",training_path)
    
    # log the config
    logger.info(cfg)    
    ###
    num_gpus = 1
    #num_gpus=torch.cuda.device_count()
    distributed = num_gpus > 1
    logger.info(f"Using Number of GPUS {num_gpus}")
    #distributed=False
    if distributed:
        torch.cuda.set_device(cfg.local_rank)
        torch.distributed.init_process_group(
            backend="nccl", init_method="env://"
        )
        synchronize()
    else:
        #set first gpu as to be used one
        torch.cuda.set_device(0)

    ##### model
    logger.info('=> creating model ...')
    model = Network(cfg)

    use_cuda = torch.cuda.is_available()
    logger.info('cuda available: {}'.format(use_cuda))
    assert use_cuda
    model = model.cuda()
    cfg.use_syncbn=False

    if cfg.use_syncbn:
        print('use sync BN')
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)

    # logger.info(model)
    logger.info('#classifier parameters: {}'.format(sum([x.nelement() for x in model.parameters()])))

    ##### optimizer
    if cfg.optim == 'Adam':
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.lr)
    elif cfg.optim == 'SGD':
        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)

    scheduler = PolyLR(optimizer, max_iter=cfg.max_iter, power=0.9, last_step=-1)

    ###
    if distributed:
        model = torch.nn.parallel.DistributedDataParallel(
            model, device_ids=[cfg.local_rank], output_device=cfg.local_rank,
            # this should be removed if we update BatchNorm stats
            broadcast_buffers=False, find_unused_parameters=True)

    ##### model_fn (criterion)
    model_fn = model_fn_decorator()

    start_iter = -1
    if cfg.pretrain:
        logger.info("=> loading checkpoint '{}'".format(cfg.pretrain))
        loaded_state_dict = torch.load(cfg.pretrain, map_location=torch.device("cpu"))['state_dict']
        model_state_dict = model.state_dict()
        #loaded_state_dict = strip_prefix_if_present(loaded, prefix="module.")
        #align_and_update_state_dicts(model_state_dict, loaded_state_dict)
        # Changed class size so load those which matches
        new_state_dict={k:v if v.size()==model_state_dict[k].size()  else  model_state_dict[k] for k,v in zip(model_state_dict.keys(), loaded_state_dict.values()) }
        model.load_state_dict(new_state_dict)
        logger.info("=> done loading")

    if cfg.resume:
        checkpoint_fn = cfg.resume
        if osp.isfile(checkpoint_fn):
            logger.info("=> loading checkpoint '{}'".format(checkpoint_fn))
            state = torch.load(checkpoint_fn, map_location=torch.device("cpu"))
            curr_iter = state['iteration'] + 1
            start_iter = curr_iter
            model_state_dict = model.state_dict()
            loaded_state_dict = strip_prefix_if_present(state['state_dict'], prefix="module.")
            align_and_update_state_dicts(model_state_dict, loaded_state_dict)
            model.load_state_dict(model_state_dict)

            scheduler = PolyLR(optimizer, max_iter=cfg.max_iter, power=0.9, last_step=curr_iter)
            optimizer.load_state_dict(state['optimizer'])

            if 'start_iter' in state:
                start_iter = state['start_iter']
            logger.info("=> loaded checkpoint '{}' (start_iter {})".format(checkpoint_fn, curr_iter))
        else:
            raise ValueError("=> no checkpoint found at '{}'".format(checkpoint_fn))

    ##### dataset
    if cfg.dataset == 'scannetv2':
        if data_name == 'scannet':
            import data.scannetv2_inst
            dataset = data.scannetv2_inst.Dataset(start_iter=start_iter)
            dataset.trainLoader()
            #dataset.valLoader()
        else:
            print("Error: no data loader - " + data_name)
            exit(0)
    if data_name == 'planteye':
        import data.planteye_inst
        dataset = data.planteye_inst.Dataset(start_iter=start_iter)
        dataset.trainLoader()
        #dataset.valLoader()
        #val_loader=dataset.val_data_loader
        
    if start_iter < 0:
        start_iter = 0

    train(dataset.train_data_loader, model, model_fn, optimizer, start_iter=start_iter,end_epoch=cfg.max_iter, scheduler=scheduler, save_to_disc=get_rank() == 0)


def main():
    start_train()


if __name__ == "__main__":
    try:
        main()
        sys.exit(0)
    except Exception as e:
    # Write out an error file. This will be returned as the failureReason in the
    # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
    # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
    # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)